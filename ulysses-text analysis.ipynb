{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "015415e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "638413d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a Pandas DataFrame\n",
    "url = 'https://raw.githubusercontent.com/kcfraser54/Hash-Table/main/ulyss12.txt'\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a4458b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# tokenize sentences \n",
    "def replace_r_with_space(text):\n",
    "    lines = text.split('\\r\\n')\n",
    "    replaced_lines = []\n",
    "    for line in lines:\n",
    "        if '\\r' in line:\n",
    "            parts = line.split('\\r')\n",
    "            replaced_line = ' '.join(parts)\n",
    "        else:\n",
    "            replaced_line = line\n",
    "        replaced_lines.append(replaced_line)\n",
    "    replaced_text = ' '.join(replaced_lines)\n",
    "    return replaced_text\n",
    "\n",
    "def split_string(text):\n",
    "    text = replace_r_with_space(text)\n",
    "    pattern = r'[.?!\\[\\]{}<>|/\\n]+'\n",
    "    split_text = re.split(pattern, text)\n",
    "    stripped_strings = [s.strip() for s in split_text]\n",
    "    return stripped_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b65f306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = split_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6688343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process_text_data(text_list):\n",
    "    tokenized_sentences = []\n",
    "    word_counts = []\n",
    "    propn_counts = []\n",
    "    noun_counts = []\n",
    "    adj_counts = []\n",
    "    verb_counts = []\n",
    "    number_counts = []\n",
    "\n",
    "    for text in text_list:\n",
    "        doc = nlp(text)\n",
    "        tokenized_sentences.append(text)\n",
    "        word_counts.append(len(doc))\n",
    "        propn_counts.append(len([token for token in doc if token.pos_ == 'PROPN']))\n",
    "        noun_counts.append(len([token for token in doc if token.pos_ == 'NOUN']))\n",
    "        adj_counts.append(len([token for token in doc if token.pos_ == 'ADJ']))\n",
    "        verb_counts.append(len([token for token in doc if token.pos_ == 'VERB']))\n",
    "        number_counts.append(len([token for token in doc if token.like_num]))\n",
    "\n",
    "    data = {\n",
    "        'text': tokenized_sentences,\n",
    "        'words': word_counts,\n",
    "        'PROPN': propn_counts,\n",
    "        'NOUN': noun_counts,\n",
    "        'ADJ': adj_counts,\n",
    "        'VERB': verb_counts,\n",
    "        'Numbers': number_counts\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a1a782b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = process_text_data(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9348a439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>Numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿ The Project Gutenberg EBook of Ulysses, by J...</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You may copy it, give it away or re-use it und...</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gutenberg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org   Title: Ulysses  Author: James Joyce  Rel...</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EBook #4300</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  words  PROPN  NOUN  ADJ   \n",
       "0  ﻿ The Project Gutenberg EBook of Ulysses, by J...     30      7     3    0  \\\n",
       "1  You may copy it, give it away or re-use it und...     29      4     2    0   \n",
       "2                                          gutenberg      1      1     0    0   \n",
       "3  org   Title: Ulysses  Author: James Joyce  Rel...     18      7     2    0   \n",
       "4                                        EBook #4300      3      1     0    0   \n",
       "\n",
       "   VERB  Numbers  \n",
       "0     0        0  \n",
       "1     6        0  \n",
       "2     0        0  \n",
       "3     0        2  \n",
       "4     0        1  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "74baec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.sort_values('words', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "32a715c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>Numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>- each and or let him pay it and invite some o...</td>\n",
       "      <td>5251</td>\n",
       "      <td>185</td>\n",
       "      <td>809</td>\n",
       "      <td>298</td>\n",
       "      <td>759</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25324</th>\n",
       "      <td>6 per doz going out to see her aunt if you ple...</td>\n",
       "      <td>4402</td>\n",
       "      <td>106</td>\n",
       "      <td>583</td>\n",
       "      <td>193</td>\n",
       "      <td>701</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25327</th>\n",
       "      <td>- in all sure you cant get on in this world wi...</td>\n",
       "      <td>4272</td>\n",
       "      <td>180</td>\n",
       "      <td>669</td>\n",
       "      <td>246</td>\n",
       "      <td>586</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>Mulveys was the first when I was in bed that m...</td>\n",
       "      <td>2999</td>\n",
       "      <td>138</td>\n",
       "      <td>484</td>\n",
       "      <td>159</td>\n",
       "      <td>447</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25331</th>\n",
       "      <td>2 a minute even if some of it went down what i...</td>\n",
       "      <td>2228</td>\n",
       "      <td>85</td>\n",
       "      <td>337</td>\n",
       "      <td>120</td>\n",
       "      <td>310</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  words  PROPN  NOUN   \n",
       "25329  - each and or let him pay it and invite some o...   5251    185   809  \\\n",
       "25324  6 per doz going out to see her aunt if you ple...   4402    106   583   \n",
       "25327  - in all sure you cant get on in this world wi...   4272    180   669   \n",
       "25328  Mulveys was the first when I was in bed that m...   2999    138   484   \n",
       "25331  2 a minute even if some of it went down what i...   2228     85   337   \n",
       "\n",
       "       ADJ  VERB  Numbers  \n",
       "25329  298   759       58  \n",
       "25324  193   701       36  \n",
       "25327  246   586       52  \n",
       "25328  159   447       32  \n",
       "25331  120   310       22  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "8b319a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define the sentiment analysis function\n",
    "def analyze_sentiment(text):\n",
    "    # Perform sentiment analysis using VADER\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Extract the compound score\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    # Classify the sentiment based on the compound score\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "fee1e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['sentiment'] = t.text.apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "4f6e6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "def analyze_emotion(text):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded_input).logits\n",
    "    probabilities = torch.softmax(logits, dim=1).tolist()[0]\n",
    "    emotion_labels = ['anger', 'joy', 'optimism', 'sadness', 'surprise', 'trust']\n",
    "    emotions = {label: prob for label, prob in zip(emotion_labels, probabilities)}\n",
    "    most_important_emotion = max(emotions, key=emotions.get)\n",
    "    return most_important_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "41c67b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['emotion'] = t.text.apply(analyze_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "6a87c9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>- each and or let him pay it and invite some o...</td>\n",
       "      <td>5251</td>\n",
       "      <td>185</td>\n",
       "      <td>809</td>\n",
       "      <td>298</td>\n",
       "      <td>759</td>\n",
       "      <td>58</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'anger': 0.12793628871440887, 'joy': 0.160932...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25324</th>\n",
       "      <td>6 per doz going out to see her aunt if you ple...</td>\n",
       "      <td>4402</td>\n",
       "      <td>106</td>\n",
       "      <td>583</td>\n",
       "      <td>193</td>\n",
       "      <td>701</td>\n",
       "      <td>36</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'anger': 0.12212774902582169, 'joy': 0.153212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25327</th>\n",
       "      <td>- in all sure you cant get on in this world wi...</td>\n",
       "      <td>4272</td>\n",
       "      <td>180</td>\n",
       "      <td>669</td>\n",
       "      <td>246</td>\n",
       "      <td>586</td>\n",
       "      <td>52</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'anger': 0.1349671632051468, 'joy': 0.1564356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25328</th>\n",
       "      <td>Mulveys was the first when I was in bed that m...</td>\n",
       "      <td>2999</td>\n",
       "      <td>138</td>\n",
       "      <td>484</td>\n",
       "      <td>159</td>\n",
       "      <td>447</td>\n",
       "      <td>32</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'anger': 0.10962546616792679, 'joy': 0.211760...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25331</th>\n",
       "      <td>2 a minute even if some of it went down what i...</td>\n",
       "      <td>2228</td>\n",
       "      <td>85</td>\n",
       "      <td>337</td>\n",
       "      <td>120</td>\n",
       "      <td>310</td>\n",
       "      <td>22</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'anger': 0.1235189139842987, 'joy': 0.1550717...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  words  PROPN  NOUN   \n",
       "25329  - each and or let him pay it and invite some o...   5251    185   809  \\\n",
       "25324  6 per doz going out to see her aunt if you ple...   4402    106   583   \n",
       "25327  - in all sure you cant get on in this world wi...   4272    180   669   \n",
       "25328  Mulveys was the first when I was in bed that m...   2999    138   484   \n",
       "25331  2 a minute even if some of it went down what i...   2228     85   337   \n",
       "\n",
       "       ADJ  VERB  Numbers sentiment   \n",
       "25329  298   759       58  Positive  \\\n",
       "25324  193   701       36  Positive   \n",
       "25327  246   586       52  Positive   \n",
       "25328  159   447       32  Positive   \n",
       "25331  120   310       22  Positive   \n",
       "\n",
       "                                                 emotion  \n",
       "25329  {'anger': 0.12793628871440887, 'joy': 0.160932...  \n",
       "25324  {'anger': 0.12212774902582169, 'joy': 0.153212...  \n",
       "25327  {'anger': 0.1349671632051468, 'joy': 0.1564356...  \n",
       "25328  {'anger': 0.10962546616792679, 'joy': 0.211760...  \n",
       "25331  {'anger': 0.1235189139842987, 'joy': 0.1550717...  "
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "7ac47cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/scattertext/termscoring/BetaPosterior.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_freq_df['score'][cat_freq_df['cat_pct'] == cat_freq_df['ncat_pct']] = 0\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/scattertext/termscoring/BetaPosterior.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_freq_df['score'][cat_freq_df['cat_pct'] < cat_freq_df['ncat_pct']] = cat_freq_df['ncat_z']\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/scattertext/termscoring/BetaPosterior.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_freq_df['score'][cat_freq_df['cat_pct'] > cat_freq_df['ncat_pct']] = -cat_freq_df['cat_z']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Fresh Terms\n",
      "      cat  ncat  all   cat_pct  ncat_pct         cat_p  ncat_p     cat_z   \n",
      "term                                                                       \n",
      "hand  270    16  286  0.003172  0.000312  2.277157e-14     1.0 -7.544097  \\\n",
      "like  426    72  498  0.005005  0.001405  4.533541e-14     1.0 -7.453827   \n",
      "good  261    21  282  0.003067  0.000410  5.877961e-13     1.0 -7.108208   \n",
      "yes   186    10  196  0.002185  0.000195  1.195054e-10     1.0 -6.333919   \n",
      "best  122     4  126  0.001433  0.000078  4.427536e-08     1.0 -5.348777   \n",
      "\n",
      "      ncat_z     score  \n",
      "term                    \n",
      "hand     inf  7.544097  \n",
      "like     inf  7.453827  \n",
      "good     inf  7.108208  \n",
      "yes      inf  6.333919  \n",
      "best     inf  5.348777  \n",
      "Top Rotten Terms\n",
      "        cat  ncat  all   cat_pct  ncat_pct  cat_p        ncat_p  cat_z   \n",
      "term                                                                     \n",
      "no      119   421  540  0.001398  0.008216    1.0  6.377762e-84    inf  \\\n",
      "poor     23   134  157  0.000270  0.002615    1.0  3.913092e-33    inf   \n",
      "dead      8   103  111  0.000094  0.002010    1.0  1.304220e-29    inf   \n",
      "bloody    6    85   91  0.000070  0.001659    1.0  4.906839e-25    inf   \n",
      "cried    11    67   78  0.000129  0.001307    1.0  9.411715e-18    inf   \n",
      "\n",
      "           ncat_z      score  \n",
      "term                          \n",
      "no     -19.374148 -19.374148  \n",
      "poor   -11.934463 -11.934463  \n",
      "dead   -11.239504 -11.239504  \n",
      "bloody -10.268082 -10.268082  \n",
      "cried   -8.500833  -8.500833  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/scattertext/termscoring/BetaPosterior.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_freq_df['score'][cat_freq_df['cat_pct'] == cat_freq_df['ncat_pct']] = 0\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/scattertext/termscoring/BetaPosterior.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_freq_df['score'][cat_freq_df['cat_pct'] < cat_freq_df['ncat_pct']] = cat_freq_df['ncat_z']\n",
      "/Users/liviaclarete/.virtualenvs/nlp/lib/python3.9/site-packages/scattertext/termscoring/BetaPosterior.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cat_freq_df['score'][cat_freq_df['cat_pct'] > cat_freq_df['ncat_pct']] = -cat_freq_df['cat_z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2912850"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scattertext as st\n",
    "\n",
    "movie_df = st.SampleCorpora.RottenTomatoes.get_data()\n",
    "\n",
    "corpus = st.CorpusFromPandas(\n",
    "    t,\n",
    "    category_col='sentiment',\n",
    "    text_col='text',\n",
    "    nlp=st.whitespace_nlp_with_sentences\n",
    ").build().get_unigram_corpus()\n",
    "\n",
    "beta_posterior = st.BetaPosterior(corpus).set_categories('Positive', ['Negative'])\n",
    "score_df = beta_posterior.get_score_df()\n",
    "print(\"Top Fresh Terms\")\n",
    "print(score_df.sort_values(by='cat_p').head())\n",
    "\n",
    "print(\"Top Rotten Terms\")\n",
    "print(score_df.sort_values(by='ncat_p').head())\n",
    "\n",
    "html = st.produce_frequency_explorer(\n",
    "    corpus,\n",
    "    category='Positive',\n",
    "    not_category_name='Negative',\n",
    "    term_scorer=beta_posterior,\n",
    "    grey_threshold=1\n",
    ")\n",
    "\n",
    "file_name = 'ulysses_sentiment_visualization.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ca504cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count_words</th>\n",
       "      <th>token_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>say</td>\n",
       "      <td>1993</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>bloom</td>\n",
       "      <td>1007</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>like</td>\n",
       "      <td>776</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>one</td>\n",
       "      <td>725</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>see</td>\n",
       "      <td>704</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>come</td>\n",
       "      <td>640</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>man</td>\n",
       "      <td>598</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>stephen</td>\n",
       "      <td>572</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>know</td>\n",
       "      <td>553</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>old</td>\n",
       "      <td>505</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  count_words token_type\n",
       "138       say         1993       VERB\n",
       "3640    bloom         1007      PROPN\n",
       "130      like          776        ADP\n",
       "157       one          725        NUM\n",
       "427       see          704       VERB\n",
       "54       come          640       VERB\n",
       "298       man          598       NOUN\n",
       "104   stephen          572      PROPN\n",
       "280      know          553       VERB\n",
       "183       old          505        ADJ"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(text, chunk_size=100000):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '')))\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Filter only numbers and words\n",
    "    filtered_text = re.findall(r'\\b[a-zA-Z0-9]+\\b', text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(' '.join(filtered_text))\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stopwords_set]\n",
    "\n",
    "    # Initialize spaCy\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    # Process text in chunks\n",
    "    lemmatized_words = []\n",
    "    token_types = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = words[i:i+chunk_size]\n",
    "        chunk_text = ' '.join(chunk)\n",
    "\n",
    "        # Lemmatize chunk using spaCy\n",
    "        doc = nlp(chunk_text)\n",
    "        chunk_lemmas = [token.lemma_ for token in doc]\n",
    "        lemmatized_words.extend(chunk_lemmas)\n",
    "        token_types.extend([token.pos_ for token in doc])\n",
    "\n",
    "    # Count the occurrence of each word\n",
    "    word_counts = {}\n",
    "    for word, token_type in zip(lemmatized_words, token_types):\n",
    "        if len(word) > 2:\n",
    "            if word in word_counts:\n",
    "                word_counts[word][0] += 1\n",
    "            else:\n",
    "                word_counts[word] = [1, token_type]\n",
    "\n",
    "    # Create a DataFrame from the word counts\n",
    "    df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count_words', 'token_type'])\n",
    "    df.index.name = 'words'\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.sort_values(by='count_words', ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_counts = count_words(text)\n",
    "\n",
    "df_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6b89682b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token_type\n",
       "PROPN    7748\n",
       "NOUN     7683\n",
       "ADJ      4092\n",
       "VERB     3066\n",
       "ADV      1072\n",
       "NUM       145\n",
       "X          99\n",
       "AUX        83\n",
       "ADP        66\n",
       "PRON       37\n",
       "INTJ       30\n",
       "SCONJ      22\n",
       "DET        17\n",
       "CCONJ      10\n",
       "PART        1\n",
       "PUNCT       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counts.token_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6347b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
